{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretend we live in a world with just 7 letters: E-D-H-L-O-R-W. \n",
    "\n",
    "We would like to teach our network to say \"helloworld\", given a starting syllable \"hel\". \n",
    "\n",
    "We can represent any character in our alphabet with a 7-vector, using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_to_encoding(letter):\n",
    "    letters = ['e','d','h','l','o','r','w']\n",
    "    vec = np.zeros((7),dtype=\"float32\")\n",
    "    vec[letters.index(letter)] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, the word \"hello\" would be represented as the follow matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for letter in \"hello\":\n",
    "    print(letter_to_encoding(letter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use a single-layer RNN.\n",
    "The network takes an input with shape 3x7xN, where 3 represents the input sequence length, 7 represents the size of the data vector associated with each input, and N is the number of samples in a training batch. \n",
    "\n",
    "The RNN layer is a Keras SimpleRNN; each of the 3 nodes of this RNN will create a 7-dimensional output (again used to represent the likelihood associated with a single character in our alphabet). Though each length-3 input will map to a length-3 output, we take only the 3rd (final) output to represent the predicted \"next character\" of our phrase. Had we left off the \"return_sequences\" parameter, the network would output only this final character (but for sake of example, it is interesting to see the entire predicted sequence).  \n",
    "\n",
    "Note that we apply a softmax activation function prior to output to normalize the raw output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rossg\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 3, 7)              105       \n",
      "=================================================================\n",
      "Total params: 105\n",
      "Trainable params: 105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(3,7,)))\n",
    "model.add(layers.SimpleRNN(7, activation=\"softmax\",return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise N at the end of the chapter asks you to think about why this network has 105 trainable parameters. As a hint, recall that the input to the RNN node is made up of the input vector, plus the hidden state, plus a bias term, and that these inputs are fully connected to the generated output.\n",
    "\n",
    "In the below cell, we create our training data. In our hypothetical world, the only phrase that exists is \"helloworld\", so we train the network on fragments of this phrase. You can see in the below example that for each three-letter portion, the output matches the next predicted letter following each letter of the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hel']\n",
      "['ell']\n",
      "[[0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "(296, 3, 7)\n"
     ]
    }
   ],
   "source": [
    "train_text = \"helloworld\"*30\n",
    "def generate_train_set(train_text, as_words=False):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(len(train_text)-4):\n",
    "        if as_words:\n",
    "            x_train += [[train_text[i:i+3]]]\n",
    "            y_train += [[train_text[i+1:i+4]]]\n",
    "\n",
    "        else:\n",
    "            x_train += [[letter_to_encoding(letter) for letter in train_text[i:i+3]]]\n",
    "            y_train += [[letter_to_encoding(letter) for letter in train_text[i+1:i+4]]]\n",
    "\n",
    "    if as_words:\n",
    "        print(x_train[0][:5])\n",
    "        print(y_train[0][:5])\n",
    "    else:\n",
    "        print(np.array(x_train)[0,:5])\n",
    "        print(np.array(x_train).shape)\n",
    "        \n",
    "    return np.array(x_train), np.array(y_train)\n",
    "    \n",
    "generate_train_set(train_text, True)\n",
    "x_train, y_train = generate_train_set(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what happens when we generate an output phrase using the untrained network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hele\n",
      "helee\n",
      "heleee\n",
      "heleeee\n",
      "heleeeee\n",
      "heleeeeee\n",
      "heleeeeeee\n"
     ]
    }
   ],
   "source": [
    "letters = ['e','d','h','l','o','r','w']\n",
    "\n",
    "seed = \"hel\"\n",
    "result = \"hel\"\n",
    "input_data = np.array([[letter_to_encoding(letter) for letter in seed]])\n",
    "model.get_weights()\n",
    "\n",
    "for i in range(7):\n",
    "    \n",
    "    out = model(input_data)\n",
    "    print_output = K.eval(out)\n",
    "    for row in print_output[0]:\n",
    "        next_letter = letters[np.argmax(row)]\n",
    "    result += next_letter\n",
    "    print(result)\n",
    "    input_data = np.array([[letter_to_encoding(letter) for letter in result[-3:]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ceb9ba1d08>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "model.fit(x_train, y_train, batch_size=24, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the network has effectively learned to generate the input sequence given a 3-letter seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hell\n",
      "hello\n",
      "hellow\n",
      "hellowo\n",
      "hellowor\n",
      "helloworl\n",
      "helloworld\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([[letter_to_encoding(letter) for letter in seed]])\n",
    "model.get_weights()\n",
    "result = \"hel\"\n",
    "for i in range(7):\n",
    "    out = model(input_data)\n",
    "    print_output = K.eval(out)\n",
    "    for row in print_output[0]:\n",
    "        next_letter = letters[np.argmax(row)]\n",
    "    result += next_letter\n",
    "    print(result)\n",
    "    input_data = np.array([[letter_to_encoding(letter) for letter in result[-3:]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
