{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnXo96Y7udP4",
    "outputId": "57ed2e70-f112-4bc4-ac04-794c410d0733"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x29b526f3e70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysJquz-R02Wj"
   },
   "source": [
    "# Attention and Transformer\n",
    "\n",
    "### Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z4Zrg9MQ11Q8"
   },
   "outputs": [],
   "source": [
    "test_sentence = 'ed+ed+ec-dc>aceabeg+bb+e<ed+ed+ec-dc>aceabe<c>bab<cde>g<fed>f<edc>e<dc>be<eeed+ed+ec-dc>aceabeg+bb+e<ed+ed+ec-dc>aceabe<c>ba'\n",
    "test_sentence2 = '>bc-ba+c-a+f+4d+8c+c-bc-ba+c-a+f+c-f+c-f+c-d+ed+c-bc-ba+c-a+f+4d+8c+c-d+c+c-d+c+c-f+4c+4f+f+f+f+rl8c+c+4d+c+c+4d+c+c+l16c+c-d+8c-8bc-ba+c-a+f+c-f+c-f+c-d+ed+c-bc-ba+c-a+f+c-f+c-f+c-d+ed+c-f+f+f+f+rl8c+c+4d+c+c+4d+c+c+l16c+c-d+8c-8bc-ba+c-a+f+c-f+c-f+c-d+ed+c-bc-ba+c-a+f+c-f+c-f+c-d+ed+<b<bbb>d+d+d+c+r8bb8>d+8c+8bbbd+d+d+c+r8bb8f+8c+8bbbd+d+d+c+l8rbd+c+<b2a+4a+l8a+b>f+2<a+8a+bg+2a+8a+b>f+2<a+8a+bg+4r4a+8a+b>f+4e4d+8ed+c+4l8c+<ba+b4>d+4c+4'\n",
    "test_sentence3 = 'o4<g+8>c+8e8<g+8>c+8e8<g+8>c+8e8<g+8>c+8e8<g+8>c+8e8<g+8>c+8e8<g+8>c+8e8<g+8>c+8e8<a8>c+8e8<a8>c+8e8<a8>d8f+8<a8>d8f+8<g+8>c8f+8<g+8>c+8e8<g+8>c+8d+8<f+8>c8d+8<c+8g+8>c+8<g+8>c+8e8<g+8>c+8e8<g+8>c+8e8<g+8>d+8f+8<g+8>d+8f+8<g+8>d+8f+8<g+8>d+8f+8<g+8>c+8e8<g+8>c+8e8<a8>c+8f+8<a8>c+8f+8<g+8b8>e8<g+8b8>e8<a8b8>d+8<a8b8>d+8<g+8b8>e8<b8>e8g+8<b8>e8g+8<b8>e8g+8<b8>f+8a8<b8>f+8a8<b8>f+8a8<b8>f+8a8<b8>e8g+8<b8>e8g+8c8f+8g+8c+8e8g+8d+8f+8g+8d+8f+8g+8e8g+8>c+8<e8g+8>c+8<d8f+8a8d8f+8a8c8f+8g+8c8f+8g+8c+8e8g+8c+8e8g+8c+8f8g+8c+8f8g+8c+8f+8a8c+8f+8a8c+8f+8a8c+8f+8a8c+8f8g+8c+8f8g+8c+8f8g+8c+8f8g+8c+8f+8a8c+8f+8a8c+8f+8a8c+8f+8a8c+8f8g+8c+8'\n",
    "test_sentence4 = 'f+8g+16e8g+8f+8e16c+4c+16a16a16a8a8a8g+8f+8e8f+8g+16e8g+8f+8e16c+4c+16e16c+16e4c+16e8c+16e4f+8f+8g+16e4e16f+8e16c+4c+16a16a16a8a8a8g+16f+8e4'\n",
    "test_sentence5 = 'f+2eg2f+l2d<a1a>f+e4gf+4df+1e1ed+4f+e4c+edc+dc-4c+ed<a1a4>def+e4gf+4d<a1a>f+e4gf+4df+1e1ed+4f+e4c+edc+dc-4c+edf+1f+4gba1af+f+4gba4egf+f+1f+4f+4ga4>c+<bad4d4>c+<bad4d4f+edegf+dc+d1'\n",
    "test_sentence6 = '>f+8f+8d8>b4b4<e4e4e8g+8g+8a8b8a8a8a8e4d4f+4f+4f+8e8e8f+8e8f+8f+8d8>b4b4<e4e4e8g+8g+8a8b8a8a8a8e4d4f+4f+4f+8e8e8f+8e4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try playing the test sentences here. https://firecomb.github.io/final%20project.html\n",
    "Do you recognize any of the melodies? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lwlkXiZ63PtP"
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "EPOCHS = 100\n",
    "\n",
    "# SEQ_SIZE is the number of words we are using as a context for the next word we want to predict\n",
    "SEQ_SIZE = 10 \n",
    "\n",
    "# Embedding dimension is the size of the embedding vector\n",
    "EMBEDDING_DIM = 80\n",
    "\n",
    "# Size of the hidden layer\n",
    "HIDDEN_DIM = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different values of sequence size. What effect does it have, what value did you choose, and why? \n",
    "What is an embedding vector and why is it used? (You may want to see how it is used later in the code.) \n",
    "Experiment with different number of training epochs. How many does it seem to take to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uu3U61_RujM-",
    "outputId": "9e2363ce-dd8c-48c9-e660-1fceb0b249ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ed+ed+ec-d', 'c'), ('d+ed+ec-dc', '>'), ('+ed+ec-dc>', 'a')]\n"
     ]
    }
   ],
   "source": [
    "# Build a list of ngrams.  \n",
    "test_sentences = [test_sentence, test_sentence2, test_sentence3, test_sentence4, test_sentence5, test_sentence6]\n",
    "ngrams = []\n",
    "for ts in test_sentences:\n",
    "    ngrams += [(ts[i:i+SEQ_SIZE], ts[i+SEQ_SIZE]) for i in range(len(ts) - SEQ_SIZE)]\n",
    "\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(ngrams[:3])\n",
    "\n",
    "vocab = list(set(test_sentence+test_sentence2+test_sentence3+test_sentence4+test_sentence5+test_sentence6)) \n",
    "word_to_ix2 = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_wxyb3btuq-p"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom self attention layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feat,out_feat):\n",
    "        super().__init__()             \n",
    "        self.Q = nn.Linear(in_feat,out_feat) # Query\n",
    "        self.K = nn.Linear(in_feat,out_feat) # Key\n",
    "        self.V = nn.Linear(in_feat,out_feat) # Value\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        Q = self.Q(x)\n",
    "        K = self.K(x)\n",
    "        V = self.V(x)\n",
    "        d = K.shape[0] # dimension of key vector\n",
    "        QK_d = (Q @ K.T)/(d)**0.5\n",
    "        prob = self.softmax(QK_d)\n",
    "        # YOUR CODE HERE \n",
    "        attention = prob @ V # remove for assignment\n",
    "        return attention\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size,seq_size,hidden):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,embed_size)\n",
    "        self.attention = Attention(embed_size,hidden)\n",
    "        self.fc1 = nn.Linear(hidden*seq_size,vocab_size) # converting n rows to 1\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.embed(x)\n",
    "        x = self.attention(x).view(1,-1)\n",
    "        x = self.fc1(x)\n",
    "        log_probs = F.log_softmax(x,dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an Embedding layer used for? \n",
    "What is the reason for calling .view(1,-1) on the output of attention?\n",
    "Fill in the code to compute the attention (one line). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IOi3abY9u0Ue",
    "outputId": "71c58502-5b1c-4856-c8d9-81fab7c9ddd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  2983.9944755349425\n",
      "Epoch:  2  Loss:  2066.2663327899063\n",
      "Epoch:  4  Loss:  1598.2062686489953\n",
      "Epoch:  6  Loss:  1365.2942089698845\n",
      "Epoch:  8  Loss:  1267.71553654118\n",
      "Epoch:  10  Loss:  1224.3861198109548\n",
      "Epoch:  12  Loss:  1236.5834749797266\n",
      "Epoch:  14  Loss:  1124.8764852002828\n",
      "Epoch:  16  Loss:  1085.7124665480492\n",
      "Epoch:  18  Loss:  1138.0183104948753\n",
      "Epoch:  20  Loss:  1025.1738257244601\n",
      "Epoch:  22  Loss:  1002.7268440382624\n",
      "Epoch:  24  Loss:  1072.015600787237\n",
      "Epoch:  26  Loss:  1177.677558836749\n",
      "Epoch:  28  Loss:  980.6713057858899\n",
      "Epoch:  30  Loss:  1022.158679061986\n",
      "Epoch:  32  Loss:  988.7639978099585\n",
      "Epoch:  34  Loss:  928.2199691419346\n",
      "Epoch:  36  Loss:  1012.9299477521133\n",
      "Epoch:  38  Loss:  974.9750729247902\n",
      "Epoch:  40  Loss:  914.6516808973531\n",
      "Epoch:  42  Loss:  945.9334840390077\n",
      "Epoch:  44  Loss:  884.5291260938827\n",
      "Epoch:  46  Loss:  852.5585296150042\n",
      "Epoch:  48  Loss:  841.357947403188\n",
      "Epoch:  50  Loss:  863.3989173922553\n",
      "Epoch:  52  Loss:  850.4280722761131\n",
      "Epoch:  54  Loss:  806.1033230961118\n",
      "Epoch:  56  Loss:  733.7766692159632\n",
      "Epoch:  58  Loss:  731.9193587302713\n",
      "Epoch:  60  Loss:  680.9791857864001\n",
      "Epoch:  62  Loss:  874.686902790608\n",
      "Epoch:  64  Loss:  723.2528190651433\n",
      "Epoch:  66  Loss:  1188.1190071570786\n",
      "Epoch:  68  Loss:  839.0765244177437\n",
      "Epoch:  70  Loss:  1064.225040127735\n",
      "Epoch:  72  Loss:  786.2882817360802\n",
      "Epoch:  74  Loss:  813.7579894857275\n",
      "Epoch:  76  Loss:  792.2275190418559\n",
      "Epoch:  78  Loss:  672.8837093445609\n",
      "Epoch:  80  Loss:  658.2472848767889\n",
      "Epoch:  82  Loss:  612.8306002514636\n",
      "Epoch:  84  Loss:  576.9965421009113\n",
      "Epoch:  86  Loss:  561.0007479910928\n",
      "Epoch:  88  Loss:  546.1607715770376\n",
      "Epoch:  90  Loss:  533.4289977502193\n",
      "Epoch:  92  Loss:  526.4456326391037\n",
      "Epoch:  94  Loss:  514.9828083703081\n",
      "Epoch:  96  Loss:  507.1680781150161\n",
      "Epoch:  98  Loss:  500.61204543916557\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.005\n",
    "loss_function = nn.NLLLoss()  # negative log likelihood\n",
    "\n",
    "model = Model(len(vocab),EMBEDDING_DIM,SEQ_SIZE,HIDDEN_DIM)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for i in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "        # context, target = ['thomas', 'edison.'] the\n",
    "        \n",
    "        # step 1: context id generation\n",
    "        context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # step 2: setting zero gradient for models\n",
    "        model.zero_grad()\n",
    "\n",
    "        # step 3: Forward propogation for calculating log probs\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # step 4: calculating loss\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix2[target]], dtype=torch.long))\n",
    "\n",
    "        # step 5: finding the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #step 6: updating the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    if i%2==0:\n",
    "        print(\"Epoch: \",str(i),\" Loss: \",str(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxFOjNlRu6xL",
    "outputId": "4eb3c232-0112-405f-8261-6c14ff01d5b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a8b8a8a8a8', 'e')\n",
      "Acutal indices:  tensor([4, 3, 2, 3, 4, 3, 4, 3, 4, 3]) tensor([13])\n",
      "Predicted indices:  tensor(13)\n",
      "tensor([[9.1245e-04, 7.3691e-03, 8.9472e-03, 2.0551e-04, 1.6736e-02, 8.8018e-06,\n",
      "         4.2876e-06, 1.3314e-10, 3.1988e-05, 1.8387e-06, 5.4732e-07, 8.2017e-08,\n",
      "         2.2492e-04, 9.6111e-01, 1.4631e-03, 2.7763e-03, 2.0386e-04, 9.3508e-07,\n",
      "         2.4967e-07]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Prediction\n",
    "with torch.no_grad():\n",
    "    # Fetching a random context and target \n",
    "    rand_val = ngrams[random.randrange(len(ngrams))]\n",
    "    print(rand_val)\n",
    "    context = rand_val[0]\n",
    "    target = rand_val[1]\n",
    "    \n",
    "    # Getting context and target index's\n",
    "    context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
    "    target_idxs = torch.tensor([word_to_ix2[w] for w in [target]], dtype=torch.long)\n",
    "    print(\"Acutal indices: \", context_idxs, target_idxs)\n",
    "    log_preds = model(context_idxs)\n",
    "    print(\"Predicted indices: \",torch.argmax(log_preds))\n",
    "    print(np.exp(log_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "lD8ZJhfqxndg",
    "outputId": "3a9c21f3-f0f8-490c-b2bc-1af51ec69b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o6ed+ed+ec-dc>aceabe<c>bab<cdb>+l8adc8c-4abad+d+f>gbbd+d+d+g+bded+d+g+lcef+d<<<c+g+f8d+8<b8bc8g+8c+8bd+8<c+8b8d+8g+8bd+8<g+8>8>e8<g+8g+8f+8<g+8g+8e8<8e8bg+8e8bd+4+4+4+8ag>b>e8g+8c8ba8g+8bc+8<g+8>8>e8<g+8g+8f+8<a8b8>d+8<g+8b8>e8g+8c8f+8<g+8b8>e8g+8c8ba8e8ad>>d+dc+14b4>d4e4e44b4<c4g+8<c+8g+8a8>e8e4e4e4fe8fc8ece4b+c-41c-ba+ef+4f+4fg+1c++f1f+ga86c+eee4g+16e8f+8<a8>c+8f+8a4bba8ec+d+4d+8c+8bd+8<c+8b8d+8g+8bdbe<g+8b\n"
     ]
    }
   ],
   "source": [
    "output_txt = []\n",
    "with torch.no_grad():\n",
    "    context = ngrams[0][0][:]\n",
    "\n",
    "    # Getting context and target index's\n",
    "    context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
    "    output_txt = context\n",
    "\n",
    "    for i in range(400):\n",
    "\n",
    "        context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
    "        log_preds = model(context_idxs)\n",
    "        ixp=torch.multinomial(np.exp(log_preds),1)\n",
    "        #ixp=torch.argmax(log_preds)\n",
    "        #print(context)\n",
    "        #print(vocab[ixp])\n",
    "        context = context[1:]+vocab[ixp]\n",
    "        #output_txt.append(vocab[ixp])\n",
    "        output_txt = output_txt + vocab[ixp]\n",
    "        #print(vocab[ixp],end=' ')\n",
    "\n",
    "output_txt = \"o6\"+\"\".join(output_txt) \n",
    "print(output_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sz_AyaEjU2G5"
   },
   "outputs": [],
   "source": [
    "torch.save(model, './atttxt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit this site (https://firecomb.github.io/final%20project.html) and play your MML text.\n",
    "Note that if your MML file will not play, there is an error in the syntax that is incompatible with the player.\n",
    "Can you locate the source of the syntax error? \n",
    "\n",
    "Complete the function below to post-process your MML output to mitigate the effects of these errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(transformer_output):\n",
    "    \n",
    "    \n",
    "    return postprocessed_transformer_output\n",
    "\n",
    "\n",
    "final_output = postprocess(output_txt)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfJTeGd08FcH"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AttentionTXT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
